{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ac8cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nehagogate/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import collections\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c963604",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reviews_unclean = []\n",
    "list_of_labels = []\n",
    "tar = tarfile.open(\"../Data/review_polarity.tar.gz\", \"r:gz\")\n",
    "counter = 0\n",
    "\n",
    "# looping through folder, \n",
    "# putting all review strings in list_of_reviews_unclean & all labels in another list_of_labels\n",
    "for member in tar.getmembers():\n",
    "    # checking that it's a normal txt file\n",
    "    if member.isreg():\n",
    "        temp_array = (member.name).split(\"/\")\n",
    "        if (len(temp_array) >= 2):\n",
    "            list_of_labels.append(temp_array[1])\n",
    "            f = tar.extractfile(member)\n",
    "            f_str = f.read().lower().decode(\"utf-8\")\n",
    "            list_of_reviews_unclean.append(f_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261a6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_num(s):\n",
    "    if re.search(r'\\d', s):\n",
    "        return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47a28267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CLEANING and PREPROCESSING THE DATA\n",
    "#  1. Removing punctuation\n",
    "#  2. Using PorterStemmer for stemming\n",
    "#  3. Removing stop words\n",
    "#  4. Removing words with length <= 2\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "list_of_reviews = []\n",
    "\n",
    "for review in list_of_reviews_unclean:\n",
    "    word_tokens = tokenizer.tokenize(review)\n",
    "    # print(pos_tag(word_tokens))\n",
    "    # preprocessed_sentence = [stemmer.stem(w.strip(\"_\")) for w in word_tokens if ((not w.lower() in stop_words) and (len(w)>2) and (check_if_num(w)==False) and (any([c!='_' for c in w]))  )]\n",
    "    preprocessed_sentence = [w[1] for w in (pos_tag(word_tokens))]\n",
    "    list_of_reviews.append(' '.join(preprocessed_sentence))\n",
    "\n",
    "# for r in (list_of_reviews[100:900]):\n",
    "#     print(r,\"\\n\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98ee6d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc cc' 'cc cd' 'cc dt' 'cc ex' 'cc fw' 'cc in' 'cc jj' 'cc jjr' 'cc jjs'\n",
      " 'cc md' 'cc nn' 'cc nnp' 'cc nns' 'cc pdt' 'cc prp' 'cc rb' 'cc rbr'\n",
      " 'cc rbs' 'cc rp' 'cc to' 'cc uh' 'cc vb' 'cc vbd' 'cc vbg' 'cc vbn'\n",
      " 'cc vbp' 'cc vbz' 'cc wdt' 'cc wp' 'cc wrb' 'cd cc' 'cd cd' 'cd dt'\n",
      " 'cd ex' 'cd in' 'cd jj' 'cd jjr' 'cd jjs' 'cd md' 'cd nn' 'cd nns'\n",
      " 'cd pdt' 'cd prp' 'cd rb' 'cd rbr' 'cd rbs' 'cd rp' 'cd to' 'cd vb'\n",
      " 'cd vbd' 'cd vbg' 'cd vbn' 'cd vbp' 'cd vbz' 'cd wdt' 'cd wp' 'cd wrb'\n",
      " 'dt cc' 'dt cd' 'dt dt' 'dt ex' 'dt fw' 'dt in' 'dt jj' 'dt jjr' 'dt jjs'\n",
      " 'dt md' 'dt nn' 'dt nnp' 'dt nnps' 'dt nns' 'dt pdt' 'dt prp' 'dt rb'\n",
      " 'dt rbr' 'dt rbs' 'dt rp' 'dt to' 'dt vb' 'dt vbd' 'dt vbg' 'dt vbn'\n",
      " 'dt vbp' 'dt vbz' 'dt wdt' 'dt wp' 'dt wrb' 'ex cc' 'ex cd' 'ex dt'\n",
      " 'ex ex' 'ex in' 'ex jj' 'ex md' 'ex nn' 'ex nns' 'ex pdt' 'ex prp'\n",
      " 'ex rb' 'ex rbs' 'ex to' 'ex vb' 'ex vbd' 'ex vbg' 'ex vbn' 'ex vbp'\n",
      " 'ex vbz' 'ex wdt' 'ex wp' 'ex wrb' 'fw cc' 'fw cd' 'fw dt' 'fw fw'\n",
      " 'fw in' 'fw jj' 'fw jjr' 'fw md' 'fw nn' 'fw nnp' 'fw nns' 'fw pdt'\n",
      " 'fw prp' 'fw rb' 'fw rp' 'fw to' 'fw vb' 'fw vbd' 'fw vbg' 'fw vbn'\n",
      " 'fw vbp' 'fw vbz' 'fw wdt' 'fw wp' 'in cc' 'in cd' 'in dt' 'in ex'\n",
      " 'in fw' 'in in' 'in jj' 'in jjr' 'in jjs' 'in md' 'in nn' 'in nnp'\n",
      " 'in nnps' 'in nns' 'in pdt' 'in prp' 'in rb' 'in rbr' 'in rbs' 'in rp'\n",
      " 'in to' 'in uh' 'in vb' 'in vbd' 'in vbg' 'in vbn' 'in vbp' 'in vbz'\n",
      " 'in wdt' 'in wp' 'in wrb' 'jj cc' 'jj cd' 'jj dt' 'jj ex' 'jj fw' 'jj in'\n",
      " 'jj jj' 'jj jjr' 'jj jjs' 'jj md' 'jj nn' 'jj nnp' 'jj nnps' 'jj nns'\n",
      " 'jj pdt' 'jj prp' 'jj rb' 'jj rbr' 'jj rbs' 'jj rp' 'jj to' 'jj uh'\n",
      " 'jj vb' 'jj vbd' 'jj vbg' 'jj vbn' 'jj vbp' 'jj vbz' 'jj wdt' 'jj wp'\n",
      " 'jj wrb' 'jjr cc' 'jjr cd' 'jjr dt' 'jjr ex' 'jjr in' 'jjr jj' 'jjr jjr'\n",
      " 'jjr jjs' 'jjr md' 'jjr nn' 'jjr nns' 'jjr pdt' 'jjr prp' 'jjr rb'\n",
      " 'jjr rbr' 'jjr rp' 'jjr to' 'jjr vb' 'jjr vbd' 'jjr vbg' 'jjr vbn'\n",
      " 'jjr vbp' 'jjr vbz' 'jjr wdt' 'jjr wp' 'jjr wrb' 'jjs cc' 'jjs cd'\n",
      " 'jjs dt' 'jjs ex' 'jjs in' 'jjs jj' 'jjs jjr' 'jjs jjs' 'jjs md' 'jjs nn'\n",
      " 'jjs nns' 'jjs pdt' 'jjs prp' 'jjs rb' 'jjs rbs' 'jjs to' 'jjs vb'\n",
      " 'jjs vbd' 'jjs vbg' 'jjs vbn' 'jjs vbp' 'jjs vbz' 'jjs wdt' 'jjs wp'\n",
      " 'jjs wrb' 'md cc' 'md cd' 'md dt' 'md ex' 'md in' 'md jj' 'md md' 'md nn'\n",
      " 'md nns' 'md prp' 'md rb' 'md rbr' 'md rbs' 'md rp' 'md to' 'md vb'\n",
      " 'md vbd' 'md vbg' 'md vbn' 'md vbz' 'md wdt' 'md wp' 'md wrb' 'nn cc'\n",
      " 'nn cd' 'nn dt' 'nn ex' 'nn fw' 'nn in' 'nn jj' 'nn jjr' 'nn jjs' 'nn md'\n",
      " 'nn nn' 'nn nnp' 'nn nnps' 'nn nns' 'nn pdt' 'nn pos' 'nn prp' 'nn rb'\n",
      " 'nn rbr' 'nn rbs' 'nn rp' 'nn sym' 'nn to' 'nn uh' 'nn vb' 'nn vbd'\n",
      " 'nn vbg' 'nn vbn' 'nn vbp' 'nn vbz' 'nn wdt' 'nn wp' 'nn wrb' 'nnp cc'\n",
      " 'nnp cd' 'nnp dt' 'nnp fw' 'nnp in' 'nnp jj' 'nnp md' 'nnp nn' 'nnp nnp'\n",
      " 'nnp nns' 'nnp rb' 'nnp rp' 'nnp vb' 'nnp vbd' 'nnp vbn' 'nnp vbp'\n",
      " 'nnp vbz' 'nnp wdt' 'nnp wp' 'nnps cc' 'nnps dt' 'nnps nn' 'nnps vbz'\n",
      " 'nns cc' 'nns cd' 'nns dt' 'nns ex' 'nns fw' 'nns in' 'nns jj' 'nns jjr'\n",
      " 'nns jjs' 'nns md' 'nns nn' 'nns nnp' 'nns nns' 'nns pdt' 'nns prp'\n",
      " 'nns rb' 'nns rbr' 'nns rbs' 'nns rp' 'nns to' 'nns uh' 'nns vb'\n",
      " 'nns vbd' 'nns vbg' 'nns vbn' 'nns vbp' 'nns vbz' 'nns wdt' 'nns wp'\n",
      " 'nns wrb' 'pdt dt' 'pdt in' 'pdt jj' 'pdt nn' 'pdt prp' 'pdt rb'\n",
      " 'pdt wdt' 'pos nn' 'prp cc' 'prp cd' 'prp dt' 'prp ex' 'prp fw' 'prp in'\n",
      " 'prp jj' 'prp jjr' 'prp jjs' 'prp md' 'prp nn' 'prp nnp' 'prp nnps'\n",
      " 'prp nns' 'prp pdt' 'prp prp' 'prp rb' 'prp rbr' 'prp rbs' 'prp rp'\n",
      " 'prp to' 'prp uh' 'prp vb' 'prp vbd' 'prp vbg' 'prp vbn' 'prp vbp'\n",
      " 'prp vbz' 'prp wdt' 'prp wp' 'prp wrb' 'rb cc' 'rb cd' 'rb dt' 'rb ex'\n",
      " 'rb fw' 'rb in' 'rb jj' 'rb jjr' 'rb jjs' 'rb md' 'rb nn' 'rb nnp'\n",
      " 'rb nns' 'rb pdt' 'rb prp' 'rb rb' 'rb rbr' 'rb rbs' 'rb rp' 'rb to'\n",
      " 'rb uh' 'rb vb' 'rb vbd' 'rb vbg' 'rb vbn' 'rb vbp' 'rb vbz' 'rb wdt'\n",
      " 'rb wp' 'rb wrb' 'rbr cc' 'rbr cd' 'rbr dt' 'rbr ex' 'rbr in' 'rbr jj'\n",
      " 'rbr jjr' 'rbr jjs' 'rbr md' 'rbr nn' 'rbr nns' 'rbr pdt' 'rbr prp'\n",
      " 'rbr rb' 'rbr rbr' 'rbr to' 'rbr vb' 'rbr vbd' 'rbr vbg' 'rbr vbn'\n",
      " 'rbr vbp' 'rbr vbz' 'rbr wdt' 'rbr wp' 'rbr wrb' 'rbs cc' 'rbs cd'\n",
      " 'rbs dt' 'rbs in' 'rbs jj' 'rbs md' 'rbs nn' 'rbs nns' 'rbs pdt'\n",
      " 'rbs prp' 'rbs rb' 'rbs to' 'rbs vb' 'rbs vbd' 'rbs vbg' 'rbs vbn'\n",
      " 'rbs vbp' 'rbs vbz' 'rbs wp' 'rp cc' 'rp cd' 'rp dt' 'rp ex' 'rp fw'\n",
      " 'rp in' 'rp jj' 'rp jjr' 'rp jjs' 'rp md' 'rp nn' 'rp nns' 'rp pdt'\n",
      " 'rp prp' 'rp rb' 'rp rbr' 'rp rbs' 'rp rp' 'rp to' 'rp vb' 'rp vbd'\n",
      " 'rp vbg' 'rp vbn' 'rp vbp' 'rp vbz' 'rp wdt' 'rp wp' 'rp wrb' 'sym nn'\n",
      " 'to cc' 'to cd' 'to dt' 'to in' 'to jj' 'to jjr' 'to jjs' 'to md' 'to nn'\n",
      " 'to nns' 'to pdt' 'to prp' 'to rb' 'to rbr' 'to rbs' 'to rp' 'to to'\n",
      " 'to vb' 'to vbd' 'to vbg' 'to vbn' 'to vbp' 'to vbz' 'to wdt' 'to wp'\n",
      " 'to wrb' 'uh cc' 'uh dt' 'uh in' 'uh jj' 'uh nn' 'uh nns' 'uh pdt'\n",
      " 'uh prp' 'uh rb' 'uh to' 'uh uh' 'uh vb' 'uh vbp' 'uh vbz' 'uh wdt'\n",
      " 'uh wrb' 'vb cc' 'vb cd' 'vb dt' 'vb ex' 'vb fw' 'vb in' 'vb jj' 'vb jjr'\n",
      " 'vb jjs' 'vb md' 'vb nn' 'vb nnp' 'vb nns' 'vb pdt' 'vb prp' 'vb rb'\n",
      " 'vb rbr' 'vb rbs' 'vb rp' 'vb to' 'vb uh' 'vb vb' 'vb vbd' 'vb vbg'\n",
      " 'vb vbn' 'vb vbp' 'vb vbz' 'vb wdt' 'vb wp' 'vb wrb' 'vbd cc' 'vbd cd'\n",
      " 'vbd dt' 'vbd ex' 'vbd fw' 'vbd in' 'vbd jj' 'vbd jjr' 'vbd jjs' 'vbd md'\n",
      " 'vbd nn' 'vbd nnp' 'vbd nns' 'vbd pdt' 'vbd prp' 'vbd rb' 'vbd rbr'\n",
      " 'vbd rbs' 'vbd rp' 'vbd to' 'vbd uh' 'vbd vb' 'vbd vbd' 'vbd vbg'\n",
      " 'vbd vbn' 'vbd vbp' 'vbd vbz' 'vbd wdt' 'vbd wp' 'vbd wrb' 'vbg cc'\n",
      " 'vbg cd' 'vbg dt' 'vbg ex' 'vbg fw' 'vbg in' 'vbg jj' 'vbg jjr' 'vbg jjs'\n",
      " 'vbg md' 'vbg nn' 'vbg nnp' 'vbg nns' 'vbg pdt' 'vbg prp' 'vbg rb'\n",
      " 'vbg rbr' 'vbg rbs' 'vbg rp' 'vbg to' 'vbg uh' 'vbg vb' 'vbg vbd'\n",
      " 'vbg vbg' 'vbg vbn' 'vbg vbp' 'vbg vbz' 'vbg wdt' 'vbg wp' 'vbg wrb'\n",
      " 'vbn cc' 'vbn cd' 'vbn dt' 'vbn ex' 'vbn fw' 'vbn in' 'vbn jj' 'vbn jjr'\n",
      " 'vbn jjs' 'vbn md' 'vbn nn' 'vbn nnp' 'vbn nns' 'vbn pdt' 'vbn prp'\n",
      " 'vbn rb' 'vbn rbr' 'vbn rbs' 'vbn rp' 'vbn to' 'vbn vb' 'vbn vbd'\n",
      " 'vbn vbg' 'vbn vbn' 'vbn vbp' 'vbn vbz' 'vbn wdt' 'vbn wp' 'vbn wrb'\n",
      " 'vbp cc' 'vbp cd' 'vbp dt' 'vbp ex' 'vbp fw' 'vbp in' 'vbp jj' 'vbp jjr'\n",
      " 'vbp jjs' 'vbp md' 'vbp nn' 'vbp nnp' 'vbp nns' 'vbp pdt' 'vbp prp'\n",
      " 'vbp rb' 'vbp rbr' 'vbp rbs' 'vbp rp' 'vbp to' 'vbp uh' 'vbp vb'\n",
      " 'vbp vbd' 'vbp vbg' 'vbp vbn' 'vbp vbp' 'vbp vbz' 'vbp wdt' 'vbp wp'\n",
      " 'vbp wrb' 'vbz cc' 'vbz cd' 'vbz dt' 'vbz ex' 'vbz fw' 'vbz in' 'vbz jj'\n",
      " 'vbz jjr' 'vbz jjs' 'vbz md' 'vbz nn' 'vbz nnp' 'vbz nns' 'vbz pdt'\n",
      " 'vbz prp' 'vbz rb' 'vbz rbr' 'vbz rbs' 'vbz rp' 'vbz to' 'vbz uh'\n",
      " 'vbz vb' 'vbz vbd' 'vbz vbg' 'vbz vbn' 'vbz vbp' 'vbz vbz' 'vbz wdt'\n",
      " 'vbz wp' 'vbz wrb' 'wdt cc' 'wdt cd' 'wdt dt' 'wdt ex' 'wdt fw' 'wdt in'\n",
      " 'wdt jj' 'wdt jjr' 'wdt jjs' 'wdt md' 'wdt nn' 'wdt nns' 'wdt pdt'\n",
      " 'wdt prp' 'wdt rb' 'wdt rbr' 'wdt to' 'wdt vb' 'wdt vbd' 'wdt vbg'\n",
      " 'wdt vbn' 'wdt vbp' 'wdt vbz' 'wdt wdt' 'wdt wp' 'wdt wrb' 'wp cc'\n",
      " 'wp cd' 'wp dt' 'wp ex' 'wp fw' 'wp in' 'wp jj' 'wp jjr' 'wp jjs' 'wp md'\n",
      " 'wp nn' 'wp nns' 'wp pdt' 'wp prp' 'wp rb' 'wp rbr' 'wp rbs' 'wp to'\n",
      " 'wp vb' 'wp vbd' 'wp vbg' 'wp vbn' 'wp vbp' 'wp vbz' 'wp wdt' 'wp wp'\n",
      " 'wp wrb' 'wrb cc' 'wrb cd' 'wrb dt' 'wrb ex' 'wrb fw' 'wrb in' 'wrb jj'\n",
      " 'wrb jjr' 'wrb jjs' 'wrb md' 'wrb nn' 'wrb nns' 'wrb pdt' 'wrb prp'\n",
      " 'wrb rb' 'wrb rbr' 'wrb to' 'wrb vb' 'wrb vbd' 'wrb vbg' 'wrb vbn'\n",
      " 'wrb vbp' 'wrb vbz' 'wrb wdt' 'wrb wp' 'wrb wrb']\n"
     ]
    }
   ],
   "source": [
    "## Using CountVectorizer\n",
    "analyzer=stemmed_words\n",
    "CountVec = CountVectorizer(ngram_range=(2,2))\n",
    "X = CountVec.fit_transform(list_of_reviews)\n",
    "# print(CountVec.get_feature_names_out())\n",
    "cv_dataframe=pd.DataFrame(X.toarray(), columns=CountVec.get_feature_names_out())\n",
    "print((CountVec.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "220e0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function borrowed from TA demo to take as input training and testing vectors and labels\n",
    "def buildClassifiers(clf, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b3f02e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAUSSIAN NAIVE BAYES: \n",
      "\tf1:  0.5882062263218579\n",
      "\tprecision:  0.5909017046100846\n",
      "\trecall:  0.5895097286226318\n",
      "\taccuracy:  0.59\n"
     ]
    }
   ],
   "source": [
    "########### GAUSSIAN NAIVE BAYES ###########\n",
    "\n",
    "y = list_of_labels\n",
    "\n",
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y)\n",
    "\n",
    "# Naive Bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculating and printing f1, precision, recall, accuracy\n",
    "f1, precision, recall = buildClassifiers(GaussianNB(), X_train, X_test, y_train, y_test )\n",
    "print(\"GAUSSIAN NAIVE BAYES: \")\n",
    "print(\"\\tf1: \", f1)\n",
    "print(\"\\tprecision: \", precision)\n",
    "print(\"\\trecall: \", recall)\n",
    "print(\"\\taccuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc466a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE CLASSIFIER: \n",
      "\tf1:  0.5456928883925534\n",
      "\tprecision:  0.5473925299506694\n",
      "\trecall:  0.5476804331362091\n",
      "\taccuracy:  0.546\n"
     ]
    }
   ],
   "source": [
    "########### Decision Tree Classifier ###########\n",
    "\n",
    "y = list_of_labels\n",
    "\n",
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y)\n",
    "\n",
    "# Decision Tree Classifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(random_state=0, criterion='gini')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculating and printing f1, precision, recall, accuracy\n",
    "f1, precision, recall = buildClassifiers(DecisionTreeClassifier(random_state=0, criterion='gini'), X_train, X_test, y_train, y_test )\n",
    "print(\"DECISION TREE CLASSIFIER: \")\n",
    "print(\"\\tf1: \", f1)\n",
    "print(\"\\tprecision: \", precision)\n",
    "print(\"\\trecall: \", recall)\n",
    "print(\"\\taccuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "822ced84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD CLASSIFIER: \n",
      "\tf1:  0.6177045280774786\n",
      "\tprecision:  0.6313920454545454\n",
      "\trecall:  0.6243697478991597\n",
      "\taccuracy:  0.63\n"
     ]
    }
   ],
   "source": [
    "########### SGD Classifier ###########\n",
    "\n",
    "y = list_of_labels\n",
    "\n",
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y)\n",
    "\n",
    "# SGD Classifier \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "classifier = SGDClassifier(loss = \"log_loss\")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculating and printing f1, precision, recall, accuracy\n",
    "f1, precision, recall = buildClassifiers(SGDClassifier(loss = \"log_loss\"), X_train, X_test, y_train, y_test )\n",
    "print(\"SGD CLASSIFIER: \")\n",
    "print(\"\\tf1: \", f1)\n",
    "print(\"\\tprecision: \", precision)\n",
    "print(\"\\trecall: \", recall)\n",
    "print(\"\\taccuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8525a5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION: \n",
      "\tf1:  0.657998631994528\n",
      "\tprecision:  0.6580025280404487\n",
      "\trecall:  0.658\n",
      "\taccuracy:  0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "########### Logistic Regression ###########\n",
    "\n",
    "y = list_of_labels\n",
    "\n",
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y)\n",
    "\n",
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver = \"saga\")\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculating and printing f1, precision, recall, accuracy\n",
    "f1, precision, recall = buildClassifiers(LogisticRegression(solver = \"saga\"), X_train, X_test, y_train, y_test )\n",
    "print(\"LOGISTIC REGRESSION: \")\n",
    "print(\"\\tf1: \", f1)\n",
    "print(\"\\tprecision: \", precision)\n",
    "print(\"\\trecall: \", recall)\n",
    "print(\"\\taccuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86bf4820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCEPTRON: \n",
      "\tf1:  0.613138321769081\n",
      "\tprecision:  0.6483372119210269\n",
      "\trecall:  0.6264720942140297\n",
      "\taccuracy:  0.628\n"
     ]
    }
   ],
   "source": [
    "########### PERCEPTRON ###########\n",
    "\n",
    "y = list_of_labels\n",
    "\n",
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y)\n",
    "\n",
    "# Perceptron \n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "classifier = Perceptron()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculating and printing f1, precision, recall, accuracy\n",
    "f1, precision, recall = buildClassifiers(Perceptron(), X_train, X_test, y_train, y_test )\n",
    "print(\"PERCEPTRON: \")\n",
    "print(\"\\tf1: \", f1)\n",
    "print(\"\\tprecision: \", precision)\n",
    "print(\"\\trecall: \", recall)\n",
    "print(\"\\taccuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ae900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
